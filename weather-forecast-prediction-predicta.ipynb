{"cells":[{"cell_type":"code","execution_count":153,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-06-21T19:15:58.252136Z","iopub.status.busy":"2024-06-21T19:15:58.251655Z","iopub.status.idle":"2024-06-21T19:15:58.271163Z","shell.execute_reply":"2024-06-21T19:15:58.269517Z","shell.execute_reply.started":"2024-06-21T19:15:58.252102Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/kaggle/input/predicta-1-0-predict-the-unpredictable/submission_key.csv\n","/kaggle/input/predicta-1-0-predict-the-unpredictable/sample_submission.csv\n","/kaggle/input/predicta-1-0-predict-the-unpredictable/historical_weather.csv\n"]}],"source":["# This Python 3 environment comes with many helpful analytics libraries installed\n","# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n","# For example, here's several helpful packages to load\n","\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the read-only \"../input/\" directory\n","# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n","\n","import os\n","for dirname, _, filenames in os.walk('/kaggle/input'):\n","    for filename in filenames:\n","        print(os.path.join(dirname, filename))\n","\n","# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n","# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"]},{"cell_type":"code","execution_count":154,"metadata":{"execution":{"iopub.execute_input":"2024-06-21T19:15:58.274508Z","iopub.status.busy":"2024-06-21T19:15:58.274042Z","iopub.status.idle":"2024-06-21T19:15:58.281775Z","shell.execute_reply":"2024-06-21T19:15:58.280247Z","shell.execute_reply.started":"2024-06-21T19:15:58.274471Z"},"trusted":true},"outputs":[],"source":["# imporing the libraries\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2024-06-21T19:15:58.283965Z"},"trusted":true},"outputs":[],"source":["# set the display option to show up t0 100 rows\n","pd.set_option('display.max_rows',100)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# importing the dataset\n","weather = pd.read_csv('/kaggle/input/predicta-1-0-predict-the-unpredictable/historical_weather.csv')\n","submission = pd.read_csv('/kaggle/input/predicta-1-0-predict-the-unpredictable/submission_key.csv')\n","\n","print(weather[:10])\n","print(submission[:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# dataset info\n","weather.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# converting the date columns to datetime\n","weather['date'] = pd.to_datetime(weather['date'])\n","\n","weather['day'] = weather['date'].dt.day\n","weather['month'] = weather['date'].dt.month\n","weather['year'] = weather['date'].dt.year\n","\n","weather = weather.drop('date', axis=1)\n","weather[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# analyzing the dataset\n","def df_analyze(dataframe):\n","    df = pd.DataFrame()\n","    cl=[]; u=[]; s=[]; nans=[]\n","    \n","    for col in dataframe.columns:\n","        cl.append(col); u.append(dataframe[col].unique()); s.append(dataframe[col].unique().size); nans.append(dataframe[col].isnull().sum()) \n","        \n","    df['Columns']=cl; df['Uniques']=u; df['Cardinality']=s; df['NaNs']=nans;\n","\n","    return df\n","\n","df_info = df_analyze(weather)\n","\n","# df_info.sort_values('NaNs', ascending=False)\n","df_info"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# missing values\n","missing_percent = pd.DataFrame((weather.isna().sum(axis=0)/len(weather)) * 100, columns=['missing percentage']).sort_values('missing percentage', ascending=False)\n","missing_percent         "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# removing the columns that has more than 70% missing value percentage\n","weather = weather.drop('snow_depth_mm', axis=1)\n","weather[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# misising values\n","def add_missing_indicators(df):\n","    for col in df.columns:\n","        if df[col].isnull().sum() > 0:\n","            df[f'{col}_missing'] = df[col].isnull().astype(int)\n","    return df\n","\n","weather = add_missing_indicators(weather)\n","weather"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# converts the city_id column from the weather DataFrame into a dictionary with city IDs as keys and their counts as values.\n","weather_df_freqency_map = weather.city_id.value_counts().to_dict()\n","weather_df_freqency_map"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# frequency encoding\n","# replacing the city IDs with their frequency\n","weather.city_id = weather.city_id.map(weather_df_freqency_map)\n","weather[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # function to calculate target encoding\n","# def target_encode(train_series, target, min_samples_leaf=1, smoothing=1):\n","#     assert len(train_series) == len(target)\n","#     temp = pd.concat([train_series, target], axis=1)\n","#     averages = temp.groupby(by=train_series.name)[target.name].agg([\"mean\", \"count\"])\n","#     smoothing = 1 / (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) / smoothing))\n","\n","#     prior = target.mean()\n","#     averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n","#     averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n","\n","#     return train_series.map(averages.to_dict()[target.name])\n","\n","# # apply target encoding\n","# weather['city_id'] = target_encode(weather['city_id'], weather['avg_temp_c'])\n","# print(weather.head(10))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# dataset info\n","weather.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# handling the misising values\n","\n","# importing the libraries\n","from sklearn.impute import KNNImputer\n","from sklearn.experimental import enable_iterative_imputer\n","from sklearn.impute import IterativeImputer"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# preparing the varibles with missing values to be imputed\n","imputing_columns = ['avg_temp_c', 'min_temp_c', 'max_temp_c', 'precipitation_mm', 'avg_wind_dir_deg', 'avg_wind_speed_kmh']\n","\n","# preparing the iterative imputer\n","iterate_imp = IterativeImputer(max_iter=10, random_state=0)\n","\n","# fit and transform the data\n","imputed_data = iterate_imp.fit_transform(weather[imputing_columns])\n","\n","# create a new DataFrame with the imputed values\n","imputed_df = pd.DataFrame(imputed_data, columns=imputing_columns)\n","\n","# replace the original columns with the imputed values\n","weather[imputing_columns] = imputed_df\n","\n","weather[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# missing values\n","missing_percent = pd.DataFrame((weather.isna().sum(axis=0)/len(weather)) * 100, columns=['missing percentage']).sort_values('missing percentage', ascending=False)\n","missing_percent     "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# plotting the distributions with box plot\n","import plotly.graph_objects as go\n","from plotly.subplots import make_subplots\n","\n","feature_set = ['city_id', 'avg_temp_c', 'min_temp_c', 'max_temp_c', 'precipitation_mm', 'avg_wind_dir_deg', 'avg_wind_speed_kmh', 'day', 'month', 'year']\n","\n","fig = make_subplots(rows=4, cols=3, subplot_titles=feature_set)\n","positions = [(1,1), (1,2), (1,3), (2,1), (2,2), (2,3), (3,1), (3,2), (3,3), (4,1), (4,2), (4,3)]\n","\n","for i, pos in zip(feature_set, positions):\n","    trace = go.Box(y=weather[i], name=i)\n","    fig.add_trace(trace, row=pos[0], col=pos[1])\n","    \n","fig.update_layout(height=900, width=1000, title_text='Box Plots for Distributions')\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# calculate the correlation matrix for the DataFrame\n","correlation_matrix = weather.corr()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# plotting the correleation analysis of the data using heatmaps\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","plt.figure(figsize=(12, 8))\n","sns.heatmap(correlation_matrix, annot=True, cmap='Purples', fmt=\".2f\")\n","plt.title('Correlation Matrix')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# calculate the absolute correlation values with the target variable\n","correlation_with_target = correlation_matrix['avg_temp_c'].abs()\n","\n","# sort the correlation values in descending order and select the top features\n","top_features = correlation_with_target.sort_values(ascending=False)[1:16]\n","\n","print(top_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["## building and tarining the model\n","\n","# importing the libraries\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from tensorflow.keras.layers import Dropout, BatchNormalization"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# prepare the dataset with the selected features\n","X = weather.drop(['avg_temp_c', 'avg_temp_c_missing'], axis=1)\n","y = weather['avg_temp_c']\n","\n","# split the dataset into training and test tests\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# normalize the data\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)\n","\n","#building the model\n","model = Sequential()\n","model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dense(128, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dense(128, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dense(128, activation='relu'))\n","model.add(BatchNormalization())\n","model.add(Dense(1, activation='linear'))\n","\n","# compile the model\n","model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n","\n","# train the model\n","history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, verbose=1)\n","\n","# evaluate the model\n","loss, mae = model.evaluate(X_test, y_test, verbose=1)\n","print(f'Test Mean Absolute Error: {mae}')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# plotting the validation loss with Plotly\n","import plotly.express as px\n","\n","fig = go.Figure()\n","fig.add_trace(go.Scatter(x=list(range(len(history.history['val_loss']))), \n","                         y=history.history['val_loss'], \n","                         mode='lines', \n","                         name='Validation Loss'))\n","fig.add_trace(go.Scatter(x=list(range(len(history.history['loss']))), \n","                         y=history.history['loss'], \n","                         mode='lines', \n","                         name='Training Loss'))\n","fig.update_layout(title='Model Loss Over Epochs',\n","                  xaxis_title='Epochs',\n","                  yaxis_title='Loss')\n","fig.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# make predictions\n","y_pred = model.predict(X_test)\n","\n","# create a DataFrame to compare predictions with actual values\n","comparison_df = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred.flatten(), 'Error': (y_test - y_pred.flatten()).abs()})\n","\n","# print the comparison DataFrame\n","print(comparison_df[:50])"]},{"cell_type":"markdown","metadata":{},"source":["**Predicting the target on the new dataset**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# importing the new dataset\n","new_df = pd.read_csv('/kaggle/input/predicta-1-0-predict-the-unpredictable/submission_key.csv')\n","\n","# converting the date columns to datetime\n","new_df['date'] = pd.to_datetime(new_df['date'])\n","\n","new_df['day'] = new_df['date'].dt.day\n","new_df['month'] = new_df['date'].dt.month\n","new_df['year'] = new_df['date'].dt.year\n","\n","new_df = new_df.drop('date', axis=1)\n","\n","# encoding the categorical variables\n","# converts the city_id column from the weatherDataFrame into a dictionary with city IDs as keys and their counts as values.\n","new_df_freqency_map = new_df.city_id.value_counts().to_dict()\n","# new_df_freqency_map\n","\n","# frequency encoding\n","# replacing the city IDs with their frequency\n","new_df.city_id = new_df.city_id.map(new_df_freqency_map)\n","new_df[:100]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["#predicting the target\n","#list of all features used in the original training\n","original_features = ['city_id', 'min_temp_c', 'max_temp_c',\n","                     'precipitation_mm', 'avg_wind_dir_deg', \n","                     'avg_wind_speed_kmh', 'day', 'month', \n","                     'year', 'min_temp_c_missing', 'max_temp_c_missing', \n","                     'precipitation_mm_missing', 'avg_wind_dir_deg_missing',\n","                     'avg_wind_speed_kmh_missing']\n","\n","# Features available in the new dataset\n","new_features = ['city_id', 'day', 'month', 'year']\n","\n","# Add missing features with default values (zeros) to the new dataset\n","for feature in original_features:\n","    if feature not in new_features:\n","        new_df[feature] = 0\n","        \n","# ensuring the columns are in the same order as the original training set\n","# new_df = new_df[original_features]\n","\n","# transform the new dataset using the previously fitted scaler\n","X_new = scaler.transform(new_df)\n","\n","# make predictions\n","new_predictions = model.predict(X_new)\n","\n","# display the first 10 predictions\n","new_predictions[:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# read the submission key file\n","submission_key = pd.read_csv('/kaggle/input/predicta-1-0-predict-the-unpredictable/submission_key.csv')\n","\n","# add the sold_qty column with the rounded predictions\n","submission_key['avg_temp_c'] = new_predictions\n","\n","# select only the ID and sold_qty columns\n","submission_key_final = submission_key[['submission_ID', 'avg_temp_c']]\n","print(submission_key_final[:20])\n","\n","# save to a new CSV file\n","submission_key_final.to_csv('submission_avg_temp_predictions.csv', index=False)\n","\n","# display the saved file for download\n","from IPython.display import FileLink\n","\n","# provide a link to download the file\n","FileLink('submission_avg_temp_predictions.csv')"]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"databundleVersionId":8892995,"sourceId":81884,"sourceType":"competition"}],"dockerImageVersionId":30732,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
